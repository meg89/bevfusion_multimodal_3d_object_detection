# BEVFusion-Based Multimodal 3D Object Detection  
**Camera + LiDAR + Radar Fusion (CPU-Friendly Implementation)**

This repository contains a lightweight, CPU-friendly implementation of a **BEVFusion-inspired multimodal 3D object detection pipeline**, developed as part of an academic project.  
The system fuses **camera, LiDAR, and radar** sensor data into a unified **Birdâ€™s-Eye View (BEV)** representation and performs detection using a **CenterNet-style detection head**.

The project is evaluated on the **nuScenes v1.0 mini dataset** using standard metrics such as **mAP** and **NDS**.

ğŸ“„ **Project Report (ACM-style):**  
See the accompanying project report for detailed methodology, experiments, and analysis.

ğŸ”— **Reference Repository:**  
https://github.com/meg89/bevfusion_multimodal_3d_object_detection

---

## ğŸš€ Features

- Multimodal sensor fusion: **Camera, LiDAR, Radar**
- Unified **BEV representation**
- **ResNet-18** camera encoder
- **PointNet-based** LiDAR and radar encoders
- **BEVFusion-style feature projection**
- **CenterNet-based detection head**
- CPU-friendly (no custom CUDA ops)
- Modular design for easy experimentation with sensor combinations

---

## Follow run_instructions.txt for instruction to reproduce the code.
# Follow demo.ipynb to run the code and see the results

## ğŸ“ Repository Structure

```
.
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ base.yaml
â”œâ”€â”€ data/
â”‚   â””â”€â”€ nuscenes/
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ best_model.pth
â”œâ”€â”€ encoders.py
â”œâ”€â”€ fusion.py
â”œâ”€â”€ train_detect.py
â”œâ”€â”€ eval.py
â”œâ”€â”€ inference.py
â”œâ”€â”€ data_converter.py
â”œâ”€â”€ data_validate.py
â”œâ”€â”€ validate_data_with_samples.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```


---

## âš™ï¸ Environment Setup

### 1ï¸âƒ£ Install Requirements

```bash
python >= 3.10
pip install -r requirements.txt
```

---

## ğŸ“¦ Dataset Preparation (nuScenes-mini)

### 2ï¸âƒ£ Download Dataset

1. Register at https://www.nuscenes.org/nuscenes#download  
2. Download **nuScenes v1.0 mini**  
3. Extract the dataset into a `data/` directory  

---

### 3ï¸âƒ£ Configure Dataset Path

Edit `configs/base.yaml` and set:
```yaml
dataset:
  root_path: /absolute/path/to/data/nuscenes
```

---

### 4ï¸âƒ£ Convert Dataset

```bash
python data_converter.py --config configs/base.yaml --split train
python data_converter.py --config configs/base.yaml --split val
python data_converter.py --config configs/base.yaml --split test
```

---

### 5ï¸âƒ£ Validate Dataset Conversion

```bash
python data_converter.py --config configs/base.yaml --show-config
python data_validate.py --config configs/base.yaml --show-config
```

---

### 6ï¸âƒ£ Inspect Dataset Samples (Optional)

```bash
python validate_data_with_samples.py --config configs/base.yaml
python validate_data_with_samples.py --samples 10
python validate_data_with_samples.py --split train --samples 5
```

---

## ğŸ‹ï¸ Training Pipeline

### 7ï¸âƒ£ Sanity Check Encoders and Fusion

```bash
python encoders.py
python fusion.py
```

---

### 8ï¸âƒ£ Train the Model

```bash
python train_detect.py train configs/base.yaml
```

---

## ğŸ“Š Evaluation

```bash
python eval.py configs/base.yaml
```

---

## ğŸ” Inference

```bash
python inference.py --model checkpoints/best_model.pth
```

---

## ğŸ§ª Experiments & Modality Configurations

| Modality | Fusion | Detection Head |
|--------|--------|----------------|
| camera_only | BEVFusion | CenterNet |
| lidar_only | BEVFusion | CenterNet |
| camera + lidar | BEVFusion | CenterNet |
| camera + lidar + radar | BEVFusion | CenterNet |

---

## ğŸ‘¤ Author

**Megha Agrawal**  
Rutgers University  

---

## ğŸ“œ License

Academic and research use only.
